# Advanced Security, Sandboxing, and Distributed Deployment

Let me elaborate on authentication/authorization, plugin sandboxing, and distributed deployment capabilities.

---

## Part 1: Authentication & Authorization System

### Auth Framework

```python
# auth/auth_system.py
from typing import Optional, List, Set, Dict
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import jwt
import bcrypt
import secrets
from pathlib import Path
import json
import logging

class Permission(Enum):
    # Plugin permissions
    PLUGIN_READ = "plugin:read"
    PLUGIN_INSTALL = "plugin:install"
    PLUGIN_UNINSTALL = "plugin:uninstall"
    PLUGIN_CONFIGURE = "plugin:configure"
    PLUGIN_PUBLISH = "plugin:publish"
    
    # Component permissions
    COMPONENT_READ = "component:read"
    COMPONENT_EXECUTE = "component:execute"
    
    # MCP permissions
    MCP_READ = "mcp:read"
    MCP_MANAGE = "mcp:manage"
    MCP_CALL_TOOLS = "mcp:call_tools"
    
    # System permissions
    SYSTEM_ADMIN = "system:admin"
    TELEMETRY_READ = "telemetry:read"
    MARKETPLACE_ADMIN = "marketplace:admin"

class Role(Enum):
    ADMIN = "admin"
    DEVELOPER = "developer"
    USER = "user"
    READONLY = "readonly"

@dataclass
class User:
    id: str
    username: str
    email: str
    password_hash: str
    roles: Set[Role] = field(default_factory=set)
    permissions: Set[Permission] = field(default_factory=set)
    created_at: datetime = field(default_factory=datetime.now)
    last_login: Optional[datetime] = None
    active: bool = True
    api_keys: List[str] = field(default_factory=list)
    
    # Plugin-specific permissions
    plugin_permissions: Dict[str, Set[Permission]] = field(default_factory=dict)

@dataclass
class Session:
    session_id: str
    user_id: str
    created_at: datetime
    expires_at: datetime
    ip_address: str
    user_agent: str
    
@dataclass
class APIKey:
    key_id: str
    key_hash: str
    user_id: str
    name: str
    permissions: Set[Permission]
    created_at: datetime
    expires_at: Optional[datetime] = None
    last_used: Optional[datetime] = None

class AuthenticationSystem:
    """Complete authentication and authorization system"""
    
    def __init__(self, secret_key: str, storage_path: Optional[Path] = None):
        self.secret_key = secret_key
        self.storage_path = storage_path or Path.home() / ".claude" / "auth"
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        self.users: Dict[str, User] = {}
        self.sessions: Dict[str, Session] = {}
        self.api_keys: Dict[str, APIKey] = {}
        
        self.logger = logging.getLogger(__name__)
        
        # Role-based permission mapping
        self.role_permissions = {
            Role.ADMIN: set(Permission),  # All permissions
            Role.DEVELOPER: {
                Permission.PLUGIN_READ,
                Permission.PLUGIN_INSTALL,
                Permission.PLUGIN_CONFIGURE,
                Permission.PLUGIN_PUBLISH,
                Permission.COMPONENT_READ,
                Permission.COMPONENT_EXECUTE,
                Permission.MCP_READ,
                Permission.MCP_MANAGE,
                Permission.MCP_CALL_TOOLS,
                Permission.TELEMETRY_READ
            },
            Role.USER: {
                Permission.PLUGIN_READ,
                Permission.COMPONENT_READ,
                Permission.COMPONENT_EXECUTE,
                Permission.MCP_READ,
                Permission.MCP_CALL_TOOLS
            },
            Role.READONLY: {
                Permission.PLUGIN_READ,
                Permission.COMPONENT_READ,
                Permission.MCP_READ,
                Permission.TELEMETRY_READ
            }
        }
        
        self._load_users()
        
    def create_user(
        self,
        username: str,
        email: str,
        password: str,
        roles: Optional[Set[Role]] = None
    ) -> User:
        """Create a new user"""
        
        # Check if user exists
        if any(u.username == username for u in self.users.values()):
            raise AuthError("Username already exists")
            
        if any(u.email == email for u in self.users.values()):
            raise AuthError("Email already exists")
            
        # Hash password
        password_hash = bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode()
        
        # Create user
        user = User(
            id=secrets.token_urlsafe(16),
            username=username,
            email=email,
            password_hash=password_hash,
            roles=roles or {Role.USER}
        )
        
        # Assign permissions based on roles
        user.permissions = self._get_permissions_for_roles(user.roles)
        
        self.users[user.id] = user
        self._save_users()
        
        self.logger.info(f"Created user: {username}")
        return user
        
    def authenticate(self, username: str, password: str) -> Optional[User]:
        """Authenticate user with username and password"""
        
        user = next((u for u in self.users.values() if u.username == username), None)
        
        if not user:
            return None
            
        if not user.active:
            raise AuthError("User account is disabled")
            
        # Verify password
        if not bcrypt.checkpw(password.encode(), user.password_hash.encode()):
            return None
            
        # Update last login
        user.last_login = datetime.now()
        self._save_users()
        
        return user
        
    def create_session(
        self,
        user: User,
        ip_address: str,
        user_agent: str,
        duration_hours: int = 24
    ) -> str:
        """Create a new session for authenticated user"""
        
        session = Session(
            session_id=secrets.token_urlsafe(32),
            user_id=user.id,
            created_at=datetime.now(),
            expires_at=datetime.now() + timedelta(hours=duration_hours),
            ip_address=ip_address,
            user_agent=user_agent
        )
        
        self.sessions[session.session_id] = session
        
        self.logger.info(f"Created session for user: {user.username}")
        return session.session_id
        
    def validate_session(self, session_id: str) -> Optional[User]:
        """Validate session and return user"""
        
        session = self.sessions.get(session_id)
        
        if not session:
            return None
            
        if datetime.now() > session.expires_at:
            del self.sessions[session_id]
            return None
            
        user = self.users.get(session.user_id)
        
        if not user or not user.active:
            return None
            
        return user
        
    def create_jwt_token(
        self,
        user: User,
        duration_hours: int = 24
    ) -> str:
        """Create JWT token for user"""
        
        payload = {
            'user_id': user.id,
            'username': user.username,
            'roles': [r.value for r in user.roles],
            'permissions': [p.value for p in user.permissions],
            'exp': datetime.utcnow() + timedelta(hours=duration_hours),
            'iat': datetime.utcnow()
        }
        
        token = jwt.encode(payload, self.secret_key, algorithm='HS256')
        return token
        
    def validate_jwt_token(self, token: str) -> Optional[User]:
        """Validate JWT token and return user"""
        
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=['HS256'])
            user_id = payload['user_id']
            
            user = self.users.get(user_id)
            
            if not user or not user.active:
                return None
                
            return user
            
        except jwt.ExpiredSignatureError:
            self.logger.warning("JWT token expired")
            return None
        except jwt.InvalidTokenError:
            self.logger.warning("Invalid JWT token")
            return None
            
    def create_api_key(
        self,
        user: User,
        name: str,
        permissions: Optional[Set[Permission]] = None,
        expires_in_days: Optional[int] = None
    ) -> tuple[str, APIKey]:
        """Create API key for user"""
        
        # Generate key
        key = f"claude_{secrets.token_urlsafe(32)}"
        key_hash = bcrypt.hashpw(key.encode(), bcrypt.gensalt()).decode()
        
        # Create API key object
        api_key = APIKey(
            key_id=secrets.token_urlsafe(16),
            key_hash=key_hash,
            user_id=user.id,
            name=name,
            permissions=permissions or user.permissions,
            created_at=datetime.now(),
            expires_at=datetime.now() + timedelta(days=expires_in_days) if expires_in_days else None
        )
        
        self.api_keys[api_key.key_id] = api_key
        user.api_keys.append(api_key.key_id)
        self._save_users()
        
        self.logger.info(f"Created API key '{name}' for user: {user.username}")
        
        # Return plaintext key (only time it's visible)
        return key, api_key
        
    def validate_api_key(self, key: str) -> Optional[tuple[User, APIKey]]:
        """Validate API key and return user and key info"""
        
        for api_key in self.api_keys.values():
            if bcrypt.checkpw(key.encode(), api_key.key_hash.encode()):
                # Check expiration
                if api_key.expires_at and datetime.now() > api_key.expires_at:
                    return None
                    
                # Get user
                user = self.users.get(api_key.user_id)
                if not user or not user.active:
                    return None
                    
                # Update last used
                api_key.last_used = datetime.now()
                
                return user, api_key
                
        return None
        
    def check_permission(
        self,
        user: User,
        permission: Permission,
        plugin_id: Optional[str] = None
    ) -> bool:
        """Check if user has specific permission"""
        
        # Admin has all permissions
        if Role.ADMIN in user.roles:
            return True
            
        # Check global permissions
        if permission in user.permissions:
            # If plugin-specific, check plugin permissions
            if plugin_id:
                plugin_perms = user.plugin_permissions.get(plugin_id, set())
                return permission in plugin_perms or permission in user.permissions
            return True
            
        return False
        
    def grant_plugin_permission(
        self,
        user: User,
        plugin_id: str,
        permissions: Set[Permission]
    ):
        """Grant specific permissions for a plugin"""
        
        if plugin_id not in user.plugin_permissions:
            user.plugin_permissions[plugin_id] = set()
            
        user.plugin_permissions[plugin_id].update(permissions)
        self._save_users()
        
    def revoke_plugin_permission(
        self,
        user: User,
        plugin_id: str,
        permissions: Set[Permission]
    ):
        """Revoke specific permissions for a plugin"""
        
        if plugin_id in user.plugin_permissions:
            user.plugin_permissions[plugin_id] -= permissions
            self._save_users()
            
    def add_role(self, user: User, role: Role):
        """Add role to user"""
        
        user.roles.add(role)
        user.permissions.update(self.role_permissions[role])
        self._save_users()
        
    def remove_role(self, user: User, role: Role):
        """Remove role from user"""
        
        user.roles.discard(role)
        # Recalculate permissions
        user.permissions = self._get_permissions_for_roles(user.roles)
        self._save_users()
        
    def _get_permissions_for_roles(self, roles: Set[Role]) -> Set[Permission]:
        """Get all permissions for given roles"""
        
        permissions = set()
        for role in roles:
            permissions.update(self.role_permissions[role])
        return permissions
        
    def _load_users(self):
        """Load users from storage"""
        
        users_file = self.storage_path / "users.json"
        
        if not users_file.exists():
            # Create default admin user
            self.create_user(
                username="admin",
                email="admin@localhost",
                password="admin123",  # Should be changed immediately
                roles={Role.ADMIN}
            )
            return
            
        try:
            with open(users_file) as f:
                data = json.load(f)
                
            for user_data in data['users']:
                user = User(
                    id=user_data['id'],
                    username=user_data['username'],
                    email=user_data['email'],
                    password_hash=user_data['password_hash'],
                    roles={Role(r) for r in user_data['roles']},
                    permissions={Permission(p) for p in user_data['permissions']},
                    active=user_data.get('active', True),
                    api_keys=user_data.get('api_keys', [])
                )
                
                # Load plugin permissions
                for plugin_id, perms in user_data.get('plugin_permissions', {}).items():
                    user.plugin_permissions[plugin_id] = {Permission(p) for p in perms}
                    
                self.users[user.id] = user
                
        except Exception as e:
            self.logger.error(f"Failed to load users: {e}")
            
    def _save_users(self):
        """Save users to storage"""
        
        users_file = self.storage_path / "users.json"
        
        data = {
            'users': [
                {
                    'id': user.id,
                    'username': user.username,
                    'email': user.email,
                    'password_hash': user.password_hash,
                    'roles': [r.value for r in user.roles],
                    'permissions': [p.value for p in user.permissions],
                    'active': user.active,
                    'api_keys': user.api_keys,
                    'plugin_permissions': {
                        plugin_id: [p.value for p in perms]
                        for plugin_id, perms in user.plugin_permissions.items()
                    }
                }
                for user in self.users.values()
            ]
        }
        
        try:
            with open(users_file, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            self.logger.error(f"Failed to save users: {e}")

class AuthError(Exception):
    pass
```

### FastAPI Integration

```python
# auth/fastapi_integration.py
from fastapi import Depends, HTTPException, status, Header
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from typing import Optional
from auth.auth_system import AuthenticationSystem, User, Permission

security = HTTPBearer()

class AuthDependency:
    """FastAPI dependency for authentication"""
    
    def __init__(self, auth_system: AuthenticationSystem):
        self.auth_system = auth_system
        
    async def __call__(
        self,
        credentials: Optional[HTTPAuthorizationCredentials] = Depends(security),
        x_api_key: Optional[str] = Header(None)
    ) -> User:
        """Authenticate request using JWT or API key"""
        
        # Try API key first
        if x_api_key:
            result = self.auth_system.validate_api_key(x_api_key)
            if result:
                user, api_key = result
                return user
                
        # Try JWT token
        if credentials:
            user = self.auth_system.validate_jwt_token(credentials.credentials)
            if user:
                return user
                
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials"
        )

class PermissionChecker:
    """Dependency to check specific permissions"""
    
    def __init__(self, required_permission: Permission, plugin_id: Optional[str] = None):
        self.required_permission = required_permission
        self.plugin_id = plugin_id
        
    def __call__(self, user: User, auth_system: AuthenticationSystem) -> User:
        """Check if user has required permission"""
        
        if not auth_system.check_permission(user, self.required_permission, self.plugin_id):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Permission denied: {self.required_permission.value}"
            )
            
        return user

# Update API endpoints with authentication
from fastapi import FastAPI, Depends
from pydantic import BaseModel

class LoginRequest(BaseModel):
    username: str
    password: str

class TokenResponse(BaseModel):
    access_token: str
    token_type: str = "bearer"

class APIKeyRequest(BaseModel):
    name: str
    expires_in_days: Optional[int] = None

class APIKeyResponse(BaseModel):
    key: str
    key_id: str
    name: str

def setup_auth_endpoints(app: FastAPI, auth_system: AuthenticationSystem):
    """Add authentication endpoints to FastAPI app"""
    
    @app.post("/api/auth/login", response_model=TokenResponse)
    async def login(request: LoginRequest):
        """Login and get JWT token"""
        
        user = auth_system.authenticate(request.username, request.password)
        
        if not user:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid username or password"
            )
            
        token = auth_system.create_jwt_token(user)
        
        return TokenResponse(access_token=token)
        
    @app.post("/api/auth/api-keys", response_model=APIKeyResponse)
    async def create_api_key(
        request: APIKeyRequest,
        user: User = Depends(AuthDependency(auth_system))
    ):
        """Create new API key"""
        
        key, api_key_obj = auth_system.create_api_key(
            user,
            request.name,
            expires_in_days=request.expires_in_days
        )
        
        return APIKeyResponse(
            key=key,
            key_id=api_key_obj.key_id,
            name=api_key_obj.name
        )
        
    @app.get("/api/auth/me")
    async def get_current_user(
        user: User = Depends(AuthDependency(auth_system))
    ):
        """Get current authenticated user"""
        
        return {
            'id': user.id,
            'username': user.username,
            'email': user.email,
            'roles': [r.value for r in user.roles],
            'permissions': [p.value for p in user.permissions]
        }
```

---

## Part 2: Plugin Sandboxing System

### Sandbox Architecture

```python
# sandbox/sandbox.py
from typing import Dict, List, Optional, Any, Set
from dataclasses import dataclass
from enum import Enum
import os
import subprocess
import tempfile
import shutil
from pathlib import Path
import resource
import signal
import psutil
import logging

class SandboxType(Enum):
    PROCESS = "process"
    DOCKER = "docker"
    FIREJAIL = "firejail"
    BUBBLEWRAP = "bubblewrap"

@dataclass
class SandboxLimits:
    """Resource limits for sandboxed execution"""
    max_memory_mb: int = 512
    max_cpu_percent: float = 50.0
    max_execution_time_sec: int = 300
    max_file_size_mb: int = 100
    max_open_files: int = 100
    max_processes: int = 10
    
@dataclass
class SandboxPermissions:
    """Permissions for sandboxed plugin"""
    allow_network: bool = False
    allow_filesystem_read: Set[str] = None
    allow_filesystem_write: Set[str] = None
    allow_subprocess: bool = False
    allow_import_modules: Set[str] = None
    
    def __post_init__(self):
        if self.allow_filesystem_read is None:
            self.allow_filesystem_read = set()
        if self.allow_filesystem_write is None:
            self.allow_filesystem_write = set()
        if self.allow_import_modules is None:
            self.allow_import_modules = {
                'json', 'math', 'datetime', 'collections',
                'itertools', 'functools', 're'
            }

class PluginSandbox:
    """Sandbox for executing plugins with resource limits and permissions"""
    
    def __init__(
        self,
        plugin_id: str,
        sandbox_type: SandboxType = SandboxType.PROCESS,
        limits: Optional[SandboxLimits] = None,
        permissions: Optional[SandboxPermissions] = None
    ):
        self.plugin_id = plugin_id
        self.sandbox_type = sandbox_type
        self.limits = limits or SandboxLimits()
        self.permissions = permissions or SandboxPermissions()
        
        self.temp_dir: Optional[Path] = None
        self.process: Optional[subprocess.Popen] = None
        self.logger = logging.getLogger(__name__)
        
    def __enter__(self):
        """Enter sandbox context"""
        self._setup_sandbox()
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit sandbox context"""
        self._cleanup_sandbox()
        
    def _setup_sandbox(self):
        """Setup sandbox environment"""
        
        # Create temporary directory
        self.temp_dir = Path(tempfile.mkdtemp(prefix=f"claude_sandbox_{self.plugin_id}_"))
        
        self.logger.info(f"Created sandbox for {self.plugin_id} at {self.temp_dir}")
        
    def _cleanup_sandbox(self):
        """Cleanup sandbox environment"""
        
        # Kill any running processes
        if self.process and self.process.poll() is None:
            try:
                self.process.terminate()
                self.process.wait(timeout=5)
            except:
                self.process.kill()
                
        # Remove temporary directory
        if self.temp_dir and self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)
            
        self.logger.info(f"Cleaned up sandbox for {self.plugin_id}")
        
    def execute(
        self,
        code: str,
        timeout: Optional[int] = None
    ) -> Dict[str, Any]:
        """Execute code in sandbox"""
        
        if self.sandbox_type == SandboxType.PROCESS:
            return self._execute_process_sandbox(code, timeout)
        elif self.sandbox_type == SandboxType.DOCKER:
            return self._execute_docker_sandbox(code, timeout)
        elif self.sandbox_type == SandboxType.FIREJAIL:
            return self._execute_firejail_sandbox(code, timeout)
        elif self.sandbox_type == SandboxType.BUBBLEWRAP:
            return self._execute_bubblewrap_sandbox(code, timeout)
        else:
            raise ValueError(f"Unsupported sandbox type: {self.sandbox_type}")
            
    def _execute_process_sandbox(
        self,
        code: str,
        timeout: Optional[int]
    ) -> Dict[str, Any]:
        """Execute in process sandbox with resource limits"""
        
        # Create wrapper script with resource limits
        wrapper_code = self._create_wrapper_script(code)
        
        # Write to temp file
        script_path = self.temp_dir / "script.py"
        with open(script_path, 'w') as f:
            f.write(wrapper_code)
            
        # Prepare environment
        env = os.environ.copy()
        
        # Restrict Python path
        env['PYTHONPATH'] = str(self.temp_dir)
        
        # Disable network if not allowed
        if not self.permissions.allow_network:
            env['http_proxy'] = 'http://127.0.0.1:1'
            env['https_proxy'] = 'http://127.0.0.1:1'
            
        # Execute with resource limits
        try:
            self.process = subprocess.Popen(
                ['python', str(script_path)],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env,
                cwd=self.temp_dir,
                preexec_fn=self._set_resource_limits
            )
            
            # Monitor resource usage
            monitor_thread = threading.Thread(
                target=self._monitor_resources,
                daemon=True
            )
            monitor_thread.start()
            
            # Wait for completion
            stdout, stderr = self.process.communicate(
                timeout=timeout or self.limits.max_execution_time_sec
            )
            
            return {
                'success': self.process.returncode == 0,
                'stdout': stdout.decode(),
                'stderr': stderr.decode(),
                'return_code': self.process.returncode
            }
            
        except subprocess.TimeoutExpired:
            self.process.kill()
            return {
                'success': False,
                'error': 'Execution timeout exceeded',
                'timeout': True
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
            
    def _create_wrapper_script(self, code: str) -> str:
        """Create wrapper script with security restrictions"""
        
        wrapper = f"""
import sys
import os
import builtins

# Restrict imports
_original_import = builtins.__import__
_allowed_modules = {repr(self.permissions.allow_import_modules)}

def _restricted_import(name, *args, **kwargs):
    if name.split('.')[0] not in _allowed_modules:
        raise ImportError(f"Import of '{{name}}' is not allowed")
    return _original_import(name, *args, **kwargs)

builtins.__import__ = _restricted_import

# Restrict file operations
if not {self.permissions.allow_filesystem_write}:
    _original_open = builtins.open
    
    def _restricted_open(file, mode='r', *args, **kwargs):
        if 'w' in mode or 'a' in mode or '+' in mode:
            raise PermissionError("Write operations not allowed")
        return _original_open(file, mode, *args, **kwargs)
    
    builtins.open = _restricted_open

# Restrict subprocess
if not {self.permissions.allow_subprocess}:
    import subprocess
    subprocess.Popen = lambda *args, **kwargs: (_ for _ in ()).throw(
        PermissionError("Subprocess execution not allowed")
    )

# Execute user code
try:
    exec('''
{code}
''')
except Exception as e:
    print(f"Error: {{e}}", file=sys.stderr)
    sys.exit(1)
"""
        return wrapper
        
    def _set_resource_limits(self):
        """Set resource limits for subprocess"""
        
        # Memory limit
        memory_bytes = self.limits.max_memory_mb * 1024 * 1024
        resource.setrlimit(resource.RLIMIT_AS, (memory_bytes, memory_bytes))
        
        # CPU time limit
        cpu_time = self.limits.max_execution_time_sec
        resource.setrlimit(resource.RLIMIT_CPU, (cpu_time, cpu_time))
        
        # File size limit
        file_size_bytes = self.limits.max_file_size_mb * 1024 * 1024
        resource.setrlimit(resource.RLIMIT_FSIZE, (file_size_bytes, file_size_bytes))
        
        # Open files limit
        resource.setrlimit(resource.RLIMIT_NOFILE, (self.limits.max_open_files, self.limits.max_open_files))
        
        # Process limit
        resource.setrlimit(resource.RLIMIT_NPROC, (self.limits.max_processes, self.limits.max_processes))
        
    def _monitor_resources(self):
        """Monitor resource usage and enforce limits"""
        
        if not self.process:
            return
            
        try:
            process = psutil.Process(self.process.pid)
            
            while self.process.poll() is None:
                # Check memory
                memory_mb = process.memory_info().rss / 1024 / 1024
                if memory_mb > self.limits.max_memory_mb:
                    self.logger.warning(f"Memory limit exceeded: {memory_mb}MB")
                    self.process.kill()
                    break
                    
                # Check CPU
                cpu_percent = process.cpu_percent(interval=1)
                if cpu_percent > self.limits.max_cpu_percent:
                    self.logger.warning(f"CPU limit exceeded: {cpu_percent}%")
                    
                time.sleep(1)
                
        except psutil.NoSuchProcess:
            pass
            
    def _execute_docker_sandbox(
        self,
        code: str,
        timeout: Optional[int]
    ) -> Dict[str, Any]:
        """Execute in Docker container sandbox"""
        
        # Create Dockerfile
        dockerfile_content = f"""
FROM python:3.11-slim

# Install minimal dependencies
RUN pip install --no-cache-dir {' '.join(self.permissions.allow_import_modules)}

# Create non-root user
RUN useradd -m -u 1000 sandbox

# Set resource limits in container
USER sandbox
WORKDIR /sandbox

# Copy code
COPY script.py /sandbox/

# Run with timeout
CMD ["timeout", "{timeout or self.limits.max_execution_time_sec}", "python", "script.py"]
"""
        
        # Write files
        dockerfile_path = self.temp_dir / "Dockerfile"
        with open(dockerfile_path, 'w') as f:
            f.write(dockerfile_content)
            
        script_path = self.temp_dir / "script.py"
        with open(script_path, 'w') as f:
            f.write(code)
            
        # Build image
        image_tag = f"claude-sandbox-{self.plugin_id}"
        
        try:
            subprocess.run(
                ['docker', 'build', '-t', image_tag, '.'],
                cwd=self.temp_dir,
                check=True,
                capture_output=True
            )
            
            # Run container with limits
            docker_args = [
                'docker', 'run', '--rm',
                '--memory', f'{self.limits.max_memory_mb}m',
                '--cpus', str(self.limits.max_cpu_percent / 100),
                '--pids-limit', str(self.limits.max_processes),
            ]
            
            # Network restrictions
            if not self.permissions.allow_network:
                docker_args.extend(['--network', 'none'])
                
            # Read-only filesystem
            if not self.permissions.allow_filesystem_write:
                docker_args.append('--read-only')
                
            docker_args.append(image_tag)
            
            result = subprocess.run(
                docker_args,
                capture_output=True,
                timeout=timeout or self.limits.max_execution_time_sec
            )
            
            return {
                'success': result.returncode == 0,
                'stdout': result.stdout.decode(),
                'stderr': result.stderr.decode(),
                'return_code': result.returncode
            }
            
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'error': 'Execution timeout exceeded',
                'timeout': True
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
        finally:
            # Cleanup image
            try:
                subprocess.run(['docker', 'rmi', image_tag], capture_output=True)
            except:
                pass
                
    def _execute_firejail_sandbox(
        self,
        code: str,
        timeout: Optional[int]
    ) -> Dict[str, Any]:
        """Execute using Firejail sandbox"""
        
        script_path = self.temp_dir / "script.py"
        with open(script_path, 'w') as f:
            f.write(code)
            
        firejail_args = [
            'firejail',
            '--quiet',
            '--private=' + str(self.temp_dir),
            '--private-tmp',
            '--private-dev',
        ]
        
        # Network restrictions
        if not self.permissions.allow_network:
            firejail_args.append('--net=none')
            
        # Resource limits
        firejail_args.extend([
            '--rlimit-as=' + str(self.limits.max_memory_mb * 1024 * 1024),
            '--rlimit-cpu=' + str(self.limits.max_execution_time_sec),
            '--rlimit-fsize=' + str(self.limits.max_file_size_mb * 1024 * 1024),
        ])
        
        firejail_args.extend(['python', str(script_path)])
        
        try:
            result = subprocess.run(
                firejail_args,
                capture_output=True,
                timeout=timeout or self.limits.max_execution_time_sec
            )
            
            return {
                'success': result.returncode == 0,
                'stdout': result.stdout.decode(),
                'stderr': result.stderr.decode(),
                'return_code': result.returncode
            }
            
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'error': 'Execution timeout exceeded',
                'timeout': True
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
            
    def _execute_bubblewrap_sandbox(
        self,
        code: str,
        timeout: Optional[int]
    ) -> Dict[str, Any]:
        """Execute using Bubblewrap (bwrap) sandbox"""
        
        script_path = self.temp_dir / "script.py"
        with open(script_path, 'w') as f:
            f.write(code)
            
        bwrap_args = [
            'bwrap',
            '--ro-bind', '/usr', '/usr',
            '--ro-bind', '/lib', '/lib',
            '--ro-bind', '/lib64', '/lib64',
            '--ro-bind', '/bin', '/bin',
            '--ro-bind', '/sbin', '/sbin',
            '--tmpfs', '/tmp',
            '--proc', '/proc',
            '--dev', '/dev',
            '--bind', str(self.temp_dir), '/sandbox',
            '--chdir', '/sandbox',
            '--unshare-all',
        ]
        
        # Network restrictions
        if not self.permissions.allow_network:
            bwrap_args.append('--unshare-net')
            
        bwrap_args.extend(['python', 'script.py'])
        
        try:
            result = subprocess.run(
                bwrap_args,
                capture_output=True,
                timeout=timeout or self.limits.max_execution_time_sec,
                preexec_fn=self._set_resource_limits
            )
            
            return {
                'success': result.returncode == 0,
                'stdout': result.stdout.decode(),
                'stderr': result.stderr.decode(),
                'return_code': result.returncode
            }
            
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'error': 'Execution timeout exceeded',
                'timeout': True
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

import threading
import time
```

### Sandbox Integration

```python
# sandbox/integration.py
from sandbox.sandbox import PluginSandbox, SandboxType, SandboxLimits, SandboxPermissions
from managers.skill_manager import SkillManager, SkillBase, SkillExecutionContext
from typing import Optional
import asyncio

class SandboxedSkillManager(SkillManager):
    """Skill manager with sandboxing support"""
    
    def __init__(
        self,
        registry,
        default_sandbox_type: SandboxType = SandboxType.PROCESS,
        default_limits: Optional[SandboxLimits] = None,
        default_permissions: Optional[SandboxPermissions] = None
    ):
        super().__init__(registry)
        self.default_sandbox_type = default_sandbox_type
        self.default_limits = default_limits or SandboxLimits()
        self.default_permissions = default_permissions or SandboxPermissions()
        
        # Plugin-specific sandbox configurations
        self.plugin_sandbox_configs: Dict[str, tuple] = {}
        
    def configure_plugin_sandbox(
        self,
        plugin_id: str,
        sandbox_type: Optional[SandboxType] = None,
        limits: Optional[SandboxLimits] = None,
        permissions: Optional[SandboxPermissions] = None
    ):
        """Configure sandbox for specific plugin"""
        
        self.plugin_sandbox_configs[plugin_id] = (
            sandbox_type or self.default_sandbox_type,
            limits or self.default_limits,
            permissions or self.default_permissions
        )
        
    async def execute_skill(
        self,
        skill_id: str,
        input_data: Any,
        metadata: Optional[Dict] = None,
        use_sandbox: bool = True
    ) -> Any:
        """Execute skill with optional sandboxing"""
        
        component = self.registry.components.get(skill_id)
        if not component:
            raise SkillNotFoundError(f"Skill '{skill_id}' not found")
            
        plugin_id = component.plugin_id
        
        if not use_sandbox:
            # Execute without sandbox
            return await super().execute_skill(skill_id, input_data, metadata)
            
        # Get sandbox configuration
        sandbox_config = self.plugin_sandbox_configs.get(
            plugin_id,
            (self.default_sandbox_type, self.default_limits, self.default_permissions)
        )
        
        sandbox_type, limits, permissions = sandbox_config
        
        # Execute in sandbox
        with PluginSandbox(plugin_id, sandbox_type, limits, permissions) as sandbox:
            # Serialize execution
            execution_code = self._generate_execution_code(
                skill_id,
                input_data,
                metadata
            )
            
            # Run in sandbox
            result = await asyncio.to_thread(
                sandbox.execute,
                execution_code
            )
            
            if not result['success']:
                raise SkillExecutionError(
                    f"Sandboxed execution failed: {result.get('error', result.get('stderr'))}"
                )
                
            # Parse result
            import json
            return json.loads(result['stdout'])
            
    def _generate_execution_code(
        self,
        skill_id: str,
        input_data: Any,
        metadata: Optional[Dict]
    ) -> str:
        """Generate Python code for sandboxed execution"""
        
        import json
        
        code = f"""
import json
import sys

# Skill execution code
skill_id = {repr(skill_id)}
input_data = {json.dumps(input_data)}
metadata = {json.dumps(metadata)}

# Import and execute skill
# (This would load the actual skill code)
result = {{'status': 'success', 'data': input_data}}

# Output result
print(json.dumps(result))
"""
        return code
```

---

## Part 3: Distributed Deployment System

### Distributed Architecture

```python
# distributed/cluster.py
from typing import Dict, List, Optional, Set
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import asyncio
import aiohttp
import hashlib
import json
import logging

class NodeRole(Enum):
    MASTER = "master"
    WORKER = "worker"
    COORDINATOR = "coordinator"

class NodeStatus(Enum):
    ONLINE = "online"
    OFFLINE = "offline"
    BUSY = "busy"
    MAINTENANCE = "maintenance"

@dataclass
class ClusterNode:
    node_id: str
    hostname: str
    port: int
    role: NodeRole
    status: NodeStatus = NodeStatus.ONLINE
    capabilities: Set[str] = field(default_factory=set)
    
    # Resource info
    cpu_count: int = 0
    memory_mb: int = 0
    available_cpu: float = 0.0
    available_memory_mb: int = 0
    
    # Plugin info
    installed_plugins: Set[str] = field(default_factory=set)
    running_tasks: int = 0
    max_tasks: int = 10
    
    # Health
    last_heartbeat: Optional[datetime] = None
    
    @property
    def address(self) -> str:
        return f"http://{self.hostname}:{self.port}"
        
    @property
    def is_available(self) -> bool:
        return (
            self.status == NodeStatus.ONLINE and
            self.running_tasks < self.max_tasks
        )

@dataclass
class DistributedTask:
    task_id: str
    plugin_id: str
    skill_id: str
    input_data: Any
    metadata: Dict
    
    # Execution info
    assigned_node: Optional[str] = None
    status: str = "pending"
    created_at: datetime = field(default_factory=datetime.now)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    
    # Result
    result: Optional[Any] = None
    error: Optional[str] = None

class ClusterManager:
    """Manage distributed cluster of Claude extension nodes"""
    
    def __init__(
        self,
        node_id: str,
        role: NodeRole,
        port: int = 8000
    ):
        self.node_id = node_id
        self.role = role
        self.port = port
        
        self.nodes: Dict[str, ClusterNode] = {}
        self.tasks: Dict[str, DistributedTask] = {}
        
        self.logger = logging.getLogger(__name__)
        
        # Heartbeat
        self.heartbeat_interval = 30
        self.heartbeat_task: Optional[asyncio.Task] = None
        
    async def start(self):
        """Start cluster manager"""
        
        self.logger.info(f"Starting cluster manager (role: {self.role.value})")
        
        # Start heartbeat
        self.heartbeat_task = asyncio.create_task(self._heartbeat_loop())
        
        # Discover nodes if coordinator/master
        if self.role in [NodeRole.MASTER, NodeRole.COORDINATOR]:
            await self._discover_nodes()
            
    async def stop(self):
        """Stop cluster manager"""
        
        if self.heartbeat_task:
            self.heartbeat_task.cancel()
            
        self.logger.info("Cluster manager stopped")
        
    async def register_node(self, node: ClusterNode):
        """Register a node in the cluster"""
        
        self.nodes[node.node_id] = node
        node.last_heartbeat = datetime.now()
        
        self.logger.info(f"Registered node: {node.node_id} ({node.address})")
        
    async def unregister_node(self, node_id: str):
        """Unregister a node from the cluster"""
        
        if node_id in self.nodes:
            del self.nodes[node_id]
            self.logger.info(f"Unregistered node: {node_id}")
            
    async def submit_task(
        self,
        plugin_id: str,
        skill_id: str,
        input_data: Any,
        metadata: Optional[Dict] = None
    ) -> str:
        """Submit a task for distributed execution"""
        
        # Create task
        task = DistributedTask(
            task_id=self._generate_task_id(),
            plugin_id=plugin_id,
            skill_id=skill_id,
            input_data=input_data,
            metadata=metadata or {}
        )
        
        self.tasks[task.task_id] = task
        
        # Schedule task
        await self._schedule_task(task)
        
        return task.task_id
        
    async def get_task_result(self, task_id: str, timeout: int = 300) -> Any:
        """Get result of a task (blocks until complete)"""
        
        task = self.tasks.get(task_id)
        if not task:
            raise ValueError(f"Task {task_id} not found")
            
        # Wait for completion
        start_time = asyncio.get_event_loop().time()
        
        while task.status not in ['completed', 'failed']:
            if asyncio.get_event_loop().time() - start_time > timeout:
                raise TimeoutError(f"Task {task_id} timed out")
                
            await asyncio.sleep(1)
            
        if task.status == 'failed':
            raise Exception(f"Task failed: {task.error}")
            
        return task.result
        
    async def _schedule_task(self, task: DistributedTask):
        """Schedule task to appropriate node"""
        
        # Find available node with plugin installed
        suitable_nodes = [
            node for node in self.nodes.values()
            if (
                node.is_available and
                task.plugin_id in node.installed_plugins
            )
        ]
        
        if not suitable_nodes:
            # No suitable node, queue task
            self.logger.warning(f"No suitable node for task {task.task_id}, queuing")
            return
            
        # Select node with least load
        selected_node = min(
            suitable_nodes,
            key=lambda n: n.running_tasks / n.max_tasks
        )
        
        # Assign task
        task.assigned_node = selected_node.node_id
        task.status = 'assigned'
        selected_node.running_tasks += 1
        
        # Send task to node
        await self._send_task_to_node(task, selected_node)
        
    async def _send_task_to_node(
        self,
        task: DistributedTask,
        node: ClusterNode
    ):
        """Send task to worker node for execution"""
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{node.address}/api/tasks/execute",
                    json={
                        'task_id': task.task_id,
                        'plugin_id': task.plugin_id,
                        'skill_id': task.skill_id,
                        'input_data': task.input_data,
                        'metadata': task.metadata
                    },
                    timeout=aiohttp.ClientTimeout(total=300)
                ) as response:
                    if response.status == 200:
                        result_data = await response.json()
                        await self._handle_task_completion(
                            task.task_id,
                            result_data
                        )
                    else:
                        await self._handle_task_failure(
                            task.task_id,
                            f"Node returned status {response.status}"
                        )
                        
        except Exception as e:
            self.logger.error(f"Failed to send task to node: {e}")
            await self._handle_task_failure(task.task_id, str(e))
            
    async def _handle_task_completion(
        self,
        task_id: str,
        result_data: Dict
    ):
        """Handle task completion"""
        
        task = self.tasks.get(task_id)
        if not task:
            return
            
        task.status = 'completed'
        task.completed_at = datetime.now()
        task.result = result_data.get('result')
        
        # Update node
        if task.assigned_node:
            node = self.nodes.get(task.assigned_node)
            if node:
                node.running_tasks -= 1
                
        self.logger.info(f"Task {task_id} completed")
        
    async def _handle_task_failure(
        self,
        task_id: str,
        error: str
    ):
        """Handle task failure"""
        
        task = self.tasks.get(task_id)
        if not task:
            return
            
        task.status = 'failed'
        task.completed_at = datetime.now()
        task.error = error
        
        # Update node
        if task.assigned_node:
            node = self.nodes.get(task.assigned_node)
            if node:
                node.running_tasks -= 1
                
        self.logger.error(f"Task {task_id} failed: {error}")
        
    async def _heartbeat_loop(self):
        """Periodic heartbeat to maintain cluster health"""
        
        while True:
            try:
                await asyncio.sleep(self.heartbeat_interval)
                
                # Send heartbeat to all nodes
                for node in list(self.nodes.values()):
                    try:
                        async with aiohttp.ClientSession() as session:
                            async with session.get(
                                f"{node.address}/health",
                                timeout=aiohttp.ClientTimeout(total=5)
                            ) as response:
                                if response.status == 200:
                                    node.last_heartbeat = datetime.now()
                                    node.status = NodeStatus.ONLINE
                                else:
                                    node.status = NodeStatus.OFFLINE
                                    
                    except Exception as e:
                        self.logger.warning(f"Heartbeat failed for {node.node_id}: {e}")
                        node.status = NodeStatus.OFFLINE
                        
                # Remove offline nodes
                offline_threshold = datetime.now() - timedelta(minutes=5)
                for node_id, node in list(self.nodes.items()):
                    if (
                        node.last_heartbeat and
                        node.last_heartbeat < offline_threshold
                    ):
                        await self.unregister_node(node_id)
                        
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Error in heartbeat loop: {e}")
                
    async def _discover_nodes(self):
        """Discover nodes in the cluster"""
        
        # Implementation would use service discovery (e.g., Consul, etcd)
        # or multicast discovery
        pass
        
    def _generate_task_id(self) -> str:
        """Generate unique task ID"""
        
        import secrets
        return f"task_{secrets.token_urlsafe(16)}"
        
    def get_cluster_stats(self) -> Dict:
        """Get cluster statistics"""
        
        total_nodes = len(self.nodes)
        online_nodes = sum(1 for n in self.nodes.values() if n.status == NodeStatus.ONLINE)
        
        total_tasks = len(self.tasks)
        pending_tasks = sum(1 for t in self.tasks.values() if t.status == 'pending')
        running_tasks = sum(1 for t in self.tasks.values() if t.status in ['assigned', 'running'])
        completed_tasks = sum(1 for t in self.tasks.values() if t.status == 'completed')
        failed_tasks = sum(1 for t in self.tasks.values() if t.status == 'failed')
        
        return {
            'nodes': {
                'total': total_nodes,
                'online': online_nodes,
                'offline': total_nodes - online_nodes
            },
            'tasks': {
                'total': total_tasks,
                'pending': pending_tasks,
                'running': running_tasks,
                'completed': completed_tasks,
                'failed': failed_tasks
            }
        }

from datetime import timedelta
```

### Worker Node Implementation

```python
# distributed/worker.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Dict, Any, Optional
import asyncio
import psutil
import socket

app = FastAPI(title="Claude Extension Worker Node")

# Global state
cluster_manager: Optional[ClusterManager] = None
skill_manager: Optional[SkillManager] = None
registry: Optional[ExtensionRegistry] = None

class TaskExecutionRequest(BaseModel):
    task_id: str
    plugin_id: str
    skill_id: str
    input_data: Any
    metadata: Dict

class NodeRegistrationRequest(BaseModel):
    master_address: str

async def initialize_worker(
    node_id: str,
    port: int,
    ext_registry,
    ext_skill_manager
):
    """Initialize worker node"""
    
    global cluster_manager, skill_manager, registry
    
    registry = ext_registry
    skill_manager = ext_skill_manager
    
    # Create cluster manager
    cluster_manager = ClusterManager(
        node_id=node_id,
        role=NodeRole.WORKER,
        port=port
    )
    
    await cluster_manager.start()

@app.post("/api/tasks/execute")
async def execute_task(request: TaskExecutionRequest):
    """Execute a task on this worker node"""
    
    if not skill_manager:
        raise HTTPException(status_code=500, detail="Worker not initialized")
        
    try:
        # Execute skill
        result = await skill_manager.execute_skill(
            request.skill_id,
            request.input_data,
            request.metadata
        )
        
        return {
            'status': 'success',
            'task_id': request.task_id,
            'result': result
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Task execution failed: {str(e)}"
        )

@app.post("/api/register")
async def register_with_master(request: NodeRegistrationRequest):
    """Register this worker with master node"""
    
    if not cluster_manager:
        raise HTTPException(status_code=500, detail="Worker not initialized")
        
    try:
        # Get node info
        node_info = ClusterNode(
            node_id=cluster_manager.node_id,
            hostname=socket.gethostname(),
            port=cluster_manager.port,
            role=NodeRole.WORKER,
            cpu_count=psutil.cpu_count(),
            memory_mb=psutil.virtual_memory().total // 1024 // 1024,
            installed_plugins=set(registry.plugins.keys()) if registry else set()
        )
        
        # Send registration to master
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{request.master_address}/api/cluster/register",
                json={
                    'node_id': node_info.node_id,
                    'hostname': node_info.hostname,
                    'port': node_info.port,
                    'role': node_info.role.value,
                    'cpu_count': node_info.cpu_count,
                    'memory_mb': node_info.memory_mb,
                    'installed_plugins': list(node_info.installed_plugins)
                }
            ) as response:
                if response.status != 200:
                    raise HTTPException(
                        status_code=500,
                        detail="Failed to register with master"
                    )
                    
        return {'status': 'registered'}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    
    return {
        'status': 'healthy',
        'node_id': cluster_manager.node_id if cluster_manager else None,
        'role': cluster_manager.role.value if cluster_manager else None,
        'cpu_percent': psutil.cpu_percent(),
        'memory_percent': psutil.virtual_memory().percent
    }
```

### Load Balancer

```python
# distributed/load_balancer.py
from typing import List, Optional
from enum import Enum
import random

class LoadBalancingStrategy(Enum):
    ROUND_ROBIN = "round_robin"
    LEAST_CONNECTIONS = "least_connections"
    WEIGHTED_RANDOM = "weighted_random"
    RESOURCE_BASED = "resource_based"

class LoadBalancer:
    """Load balancer for distributing tasks across nodes"""
    
    def __init__(
        self,
        strategy: LoadBalancingStrategy = LoadBalancingStrategy.LEAST_CONNECTIONS
    ):
        self.strategy = strategy
        self.round_robin_index = 0
        
    def select_node(
        self,
        nodes: List[ClusterNode],
        task: DistributedTask
    ) -> Optional[ClusterNode]:
        """Select best node for task based on strategy"""
        
        # Filter available nodes
        available = [n for n in nodes if n.is_available]
        
        if not available:
            return None
            
        if self.strategy == LoadBalancingStrategy.ROUND_ROBIN:
            return self._round_robin_select(available)
        elif self.strategy == LoadBalancingStrategy.LEAST_CONNECTIONS:
            return self._least_connections_select(available)
        elif self.strategy == LoadBalancingStrategy.WEIGHTED_RANDOM:
            return self._weighted_random_select(available)
        elif self.strategy == LoadBalancingStrategy.RESOURCE_BASED:
            return self._resource_based_select(available, task)
        else:
            return available[0]
            
    def _round_robin_select(
        self,
        nodes: List[ClusterNode]
    ) -> ClusterNode:
        """Round-robin selection"""
        
        node = nodes[self.round_robin_index % len(nodes)]
        self.round_robin_index += 1
        return node
        
    def _least_connections_select(
        self,
        nodes: List[ClusterNode]
    ) -> ClusterNode:
        """Select node with least active connections"""
        
        return min(nodes, key=lambda n: n.running_tasks)
        
    def _weighted_random_select(
        self,
        nodes: List[ClusterNode]
    ) -> ClusterNode:
        """Weighted random selection based on available resources"""
        
        weights = [
            n.available_cpu * n.available_memory_mb
            for n in nodes
        ]
        
        total_weight = sum(weights)
        if total_weight == 0:
            return random.choice(nodes)
            
        r = random.uniform(0, total_weight)
        cumulative = 0
        
        for node, weight in zip(nodes, weights):
            cumulative += weight
            if r <= cumulative:
                return node
                
        return nodes[-1]
        
    def _resource_based_select(
        self,
        nodes: List[ClusterNode],
        task: DistributedTask
    ) -> ClusterNode:
        """Select node based on resource requirements"""
        
        # Score nodes based on available resources
        def score_node(node: ClusterNode) -> float:
            cpu_score = node.available_cpu / node.cpu_count if node.cpu_count > 0 else 0
            memory_score = node.available_memory_mb / node.memory_mb if node.memory_mb > 0 else 0
            load_score = 1 - (node.running_tasks / node.max_tasks)
            
            return (cpu_score + memory_score + load_score) / 3
            
        return max(nodes, key=score_node)
```

This comprehensive implementation provides:

 **Complete authentication system** with JWT, API keys, and RBAC
 **Advanced plugin sandboxing** with multiple isolation strategies
 **Distributed cluster management** for horizontal scaling
 **Load balancing** with multiple strategies
 **Worker node implementation** for distributed execution
 **Resource monitoring and limits** for security
 **Permission-based access control** at plugin and component level

The system is now enterprise-ready with security, isolation, and scalability. Would you like me to add monitoring dashboards, auto-scaling capabilities, or disaster recovery features?